[{"content":"Introduction to 1D Density Functional Theory Density Functional Theory (DFT) is a powerful computational method used in quantum chemistry and solid-state physics to investigate the electronic structure of many-body systems. While real-world applications of DFT often deal with three-dimensional systems, exploring DFT in one dimension provides an excellent starting point for understanding the fundamental concepts and implementation details.\nIn this blog post, we\u0026rsquo;ll walk through the process of building a 1D DFT code using Python. We\u0026rsquo;ll focus on a simple system - a particle in a box - to illustrate the key components of DFT calculations.\nTheoretical Background Before diving into the code, let\u0026rsquo;s briefly review the theoretical foundations of DFT in the context of a 1D system.\nThe core idea of DFT is that the ground state properties of a many-electron system can be determined using the electron density $n(x)$, rather than the many-body wavefunction. This is based on the Hohenberg-Kohn theorems, which state that:\nThe external potential, and hence the total energy, is a unique functional of the electron density. The ground state energy can be obtained variationally: the density that minimizes the total energy is the exact ground state density. In the Kohn-Sham formulation of DFT, we map the interacting system onto a fictitious system of non-interacting particles that generate the same density. The Kohn-Sham equations for a 1D system are:\n$$ \\left[-\\frac{1}{2}\\frac{d^2}{dx^2} + v_{eff}(x)\\right]\\phi_i(x) = \\epsilon_i\\phi_i(x) $$\nwhere $\\phi_i(x)$ are the Kohn-Sham orbitals, $\\epsilon_i$ are their energies, and $v_{eff}(x)$ is the effective potential:\n$$ v_{eff}(x) = v_{ext}(x) + v_H(x) + v_{xc}(x) $$\nHere, $v_{ext}(x)$ is the external potential, $v_H(x)$ is the Hartree potential, and $v_{xc}(x)$ is the exchange-correlation potential.\nImplementation Overview Our 1D DFT code will consist of several key components:\nBasis set definition Hamiltonian construction Density calculation Potential calculation Self-consistent field (SCF) loop We\u0026rsquo;ll implement these components step by step, explaining the purpose and functionality of each part.\nSetting Up the Environment First, let\u0026rsquo;s import the necessary Python libraries:\n1 %run jupyter_image_saver.py 1 2 3 import numpy as np from scipy import linalg import matplotlib.pyplot as plt Basis Set Definition For our 1D DFT code, we\u0026rsquo;ll use a simple basis set of sine functions, which are eigenfunctions of the particle in a box:\n$$ \\chi_n(x) = \\sqrt{\\frac{2}{L}} \\sin\\left(\\frac{n\\pi x}{L}\\right) $$\nwhere $L$ is the box length and $n$ is the basis function index.\nThe Kohn-Sham orbital $\\phi_i$ is expressed as\n$$ \\phi_i(x) = \\sum_n c_{in}\\chi_n(x) $$\nLet\u0026rsquo;s implement this basis set:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def basis_function(x: np.ndarray, n: int, L: float) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34; Compute the nth basis function for a particle in a box. Args: x (np.ndarray): Spatial grid points. n (int): Basis function index (quantum number). L (float): Length of the box. Returns: np.ndarray: Values of the nth basis function at the given grid points. \u0026#34;\u0026#34;\u0026#34; return np.sqrt(2/L) * np.sin(n * np.pi * x / L) def create_basis_set(x: np.ndarray, num_basis_functions: int, L: float) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34; Create a set of basis functions for a particle in a box. Args: x (np.ndarray): Spatial grid points. num_basis_functions (int): Number of basis functions to generate. L (float): Length of the box. Returns: np.ndarray: 2D array where each column is a basis function evaluated at the grid points. \u0026#34;\u0026#34;\u0026#34; basis_set = np.zeros((len(x), num_basis_functions)) for n in range(1, num_basis_functions + 1): basis_set[:, n-1] = basis_function(x, n, L) return basis_set Hamiltonian Construction The next step is to construct the Hamiltonian matrix. In the basis of sine functions, the kinetic energy term is diagonal:\n$$ T_{mn} = -\\frac{1}{2}\\int_0^L \\chi_m(x)\\frac{d^2}{dx^2} \\chi_n(x)dx = \\frac{n^2\\pi^2}{2L^2}\\delta_{mn} $$\nThe potential energy term needs to be integrated numerically:\n$$ V_{mn} = \\int_0^L \\chi_m(x) v_{eff}(x) \\chi_n(x) dx $$\nLet\u0026rsquo;s implement the Hamiltonian construction:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def kinetic_energy_matrix(num_basis_functions: int, L: float) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34; Construct the kinetic energy matrix for a particle in a box. Args: num_basis_functions (int): Number of basis functions. L (float): Length of the box. Returns: np.ndarray: Kinetic energy matrix. \u0026#34;\u0026#34;\u0026#34; T = np.zeros((num_basis_functions, num_basis_functions)) for n in range(1, num_basis_functions + 1): T[n-1, n-1] = (n**2 * np.pi**2) / (2 * L**2) return T def potential_energy_matrix(basis_set: np.ndarray, v_eff: np.ndarray, x: np.ndarray) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34; Construct the potential energy matrix. Args: basis_set (np.ndarray): Set of basis functions evaluated at grid points. v_eff (np.ndarray): Effective potential at grid points. x (np.ndarray): Spatial grid points. Returns: np.ndarray: Potential energy matrix. \u0026#34;\u0026#34;\u0026#34; V = np.zeros((basis_set.shape[1], basis_set.shape[1])) for m in range(basis_set.shape[1]): for n in range(basis_set.shape[1]): V[m, n] = np.trapz(basis_set[:, m] * v_eff * basis_set[:, n], x) return V def construct_hamiltonian(T: np.ndarray, V: np.ndarray) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34; Construct the Hamiltonian matrix. Args: T (np.ndarray): Kinetic energy matrix. V (np.ndarray): Potential energy matrix. Returns: np.ndarray: Hamiltonian matrix. \u0026#34;\u0026#34;\u0026#34; return T + V Density Calculation The electron density is a crucial quantity in DFT. For our 1D system, we can calculate it as:\n$$ n(x) = \\sum_i |\\phi_i(x)|^2 $$\nwhere $\\phi_i(x)$ are the Kohn-Sham orbitals. In our basis set representation, we can express this as:\n$$ n(x) = \\sum_i \\left|\\sum_n c_{in} \\chi_n(x)\\right|^2 $$\nwhere $c_{in}$ are the expansion coefficients of the Kohn-Sham orbitals in our basis.\nLet\u0026rsquo;s implement the density calculation:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def calculate_density(coefficients: np.ndarray, basis_set: np.ndarray, num_electrons: int) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34; Calculate the electron density. Args: coefficients (np.ndarray): Expansion coefficients of the Kohn-Sham orbitals. basis_set (np.ndarray): Set of basis functions evaluated at grid points. num_electrons (int): Number of electrons in the system. Returns: np.ndarray: Electron density at grid points. \u0026#34;\u0026#34;\u0026#34; occupied_orbitals = coefficients[:, :num_electrons] density = np.sum(np.abs(basis_set @ occupied_orbitals)**2, axis=1) return density Potential Calculation The effective potential in DFT consists of three terms: the external potential, the Hartree potential, and the exchange-correlation potential. For our 1D system, we\u0026rsquo;ll use a simple external potential (e.g., a harmonic oscillator potential) and a local density approximation (LDA) for the exchange-correlation potential.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def external_potential(x: np.ndarray, k: float = 1) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34; Calculate the external potential (harmonic oscillator). Args: x (np.ndarray): Spatial grid points. k (float, optional): Spring constant. Defaults to 1. Returns: np.ndarray: External potential at grid points. \u0026#34;\u0026#34;\u0026#34; return 0.5 * k * x**2 def hartree_potential(density: np.ndarray, x: np.ndarray) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34; Calculate the Hartree potential by solving Poisson\u0026#39;s equation. Args: density (np.ndarray): Electron density at grid points. x (np.ndarray): Spatial grid points. Returns: np.ndarray: Hartree potential at grid points. \u0026#34;\u0026#34;\u0026#34; v_H = np.zeros_like(density) for i in range(len(x)): v_H[i] = -4 * np.pi * np.trapz(np.abs(x[i] - x) * density, x) return v_H def exchange_correlation_potential_lda(density: np.ndarray) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34; Calculate the exchange-correlation potential using Local Density Approximation (LDA). Args: density (np.ndarray): Electron density at grid points. Returns: np.ndarray: Exchange-correlation potential at grid points. \u0026#34;\u0026#34;\u0026#34; return -((3/np.pi) * density)**(1/3) def calculate_effective_potential(density: np.ndarray, x: np.ndarray) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34; Calculate the effective potential for the Kohn-Sham equations. Args: density (np.ndarray): Electron density at grid points. x (np.ndarray): Spatial grid points. Returns: np.ndarray: Effective potential at grid points. \u0026#34;\u0026#34;\u0026#34; v_ext = external_potential(x) v_H = hartree_potential(density, x) v_xc = exchange_correlation_potential_lda(density) return v_ext + v_H + v_xc Self-Consistent Field Loop The heart of the DFT calculation is the self-consistent field (SCF) loop. We start with an initial guess for the density, calculate the effective potential, solve the Kohn-Sham equations, and then calculate a new density. We repeat this process until the density (or energy) converges.\nHere\u0026rsquo;s the implementation of the SCF loop:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def scf_loop(x: np.ndarray, num_electrons: int, num_basis_functions: int, max_iterations: int = 100, mixing_param: float = 0.5, convergence_threshold: float = 1e-6) -\u0026gt; tuple[np.ndarray, np.ndarray, np.ndarray]: \u0026#34;\u0026#34;\u0026#34; Perform the self-consistent field loop for DFT calculations. Args: x (np.ndarray): Spatial grid points. num_electrons (int): Number of electrons in the system. num_basis_functions (int): Number of basis functions to use. max_iterations (int, optional): Maximum number of SCF iterations. Defaults to 100. mixing_param (float, optional): Mixing parameter for density update. Defaults to 0.5. convergence_threshold (float, optional): Convergence threshold for density. Defaults to 1e-6. Returns: tuple[np.ndarray, np.ndarray, np.ndarray]: Final density, eigenvalues, and eigenvectors. \u0026#34;\u0026#34;\u0026#34; L = x[-1] - x[0] basis_set = create_basis_set(x, num_basis_functions, L) T = kinetic_energy_matrix(num_basis_functions, L) # Initial guess for density density = np.ones_like(x) * num_electrons / L for iteration in range(max_iterations): v_eff = calculate_effective_potential(density, x) V = potential_energy_matrix(basis_set, v_eff, x) H = construct_hamiltonian(T, V) # Solve the eigenvalue problem eigenvalues, eigenvectors = linalg.eigh(H) # Calculate new density new_density = calculate_density(eigenvectors, basis_set, num_electrons) # Check for convergence if np.max(np.abs(new_density - density)) \u0026lt; convergence_threshold: print(f\u0026#34;Converged after {iteration + 1} iterations\u0026#34;) break # Mix old and new densities density = mixing_param * new_density + (1 - mixing_param) * density return density, eigenvalues, eigenvectors Putting It All Together Now that we have all the components, let\u0026rsquo;s create a function to run the entire DFT calculation:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def run_dft_calculation(num_points: int = 1000, num_electrons: int = 2, num_basis_functions: int = 10) -\u0026gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]: \u0026#34;\u0026#34;\u0026#34; Run a complete 1D DFT calculation. Args: num_points (int, optional): Number of spatial grid points. Defaults to 1000. num_electrons (int, optional): Number of electrons in the system. Defaults to 2. num_basis_functions (int, optional): Number of basis functions to use. Defaults to 10. Returns: tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]: Spatial grid, final density, effective potential, eigenvalues, and eigenvectors. \u0026#34;\u0026#34;\u0026#34; x = np.linspace(-5, 5, num_points) density, eigenvalues, eigenvectors = scf_loop(x, num_electrons, num_basis_functions) # Calculate final effective potential v_eff = calculate_effective_potential(density, x) return x, density, v_eff, eigenvalues, eigenvectors # Run the calculation x, density, v_eff, eigenvalues, eigenvectors = run_dft_calculation() Visualization Finally, let\u0026rsquo;s visualize our results:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def visualize_results(x: np.ndarray, density: np.ndarray, v_eff: np.ndarray, eigenvectors: np.ndarray, num_basis_functions: int): \u0026#34;\u0026#34;\u0026#34; Visualize the results of the DFT calculation. Args: x (np.ndarray): Spatial grid points. density (np.ndarray): Electron density at grid points. v_eff (np.ndarray): Effective potential at grid points. eigenvectors (np.ndarray): Eigenvectors (Kohn-Sham orbitals). num_basis_functions (int): Number of basis functions used. \u0026#34;\u0026#34;\u0026#34; plt.figure(figsize=(12, 8)) plt.subplot(2, 1, 1) plt.plot(x, density, label=\u0026#39;Electron density\u0026#39;) plt.plot(x, v_eff, label=\u0026#39;Effective potential\u0026#39;) plt.legend() plt.title(\u0026#39;1D DFT Results\u0026#39;) plt.ylabel(\u0026#39;Density / Potential\u0026#39;) plt.subplot(2, 1, 2) basis_set = create_basis_set(x, num_basis_functions, x[-1] - x[0]) for i in range(3): # Plot first 3 orbitals orbital = np.sum(eigenvectors[:, i][None, :] * basis_set, axis=1) plt.plot(x, orbital, label=f\u0026#39;Orbital {i+1}\u0026#39;) plt.legend() plt.xlabel(\u0026#39;Position\u0026#39;) plt.ylabel(\u0026#39;Orbital amplitude\u0026#39;) plt.tight_layout() plt.show() # Visualize the results # visualize_results(x, density, v_eff, eigenvectors, num_basis_functions=10) This visualization function creates a figure with two subplots:\nThe first subplot shows the electron density and the effective potential. The second subplot displays the first three Kohn-Sham orbitals. Modeling H₂ with 1D DFT While our 1D DFT code is a simplification, we can adapt it to model a simple diatomic molecule like H₂. Here\u0026rsquo;s how we could approach this:\nAdapting the External Potential Instead of a harmonic oscillator potential, we need to represent two protons at fixed positions. In 1D, we can model this as:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def external_potential_H2(x: np.ndarray, R: float) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34; External potential for H₂ molecule in 1D. Args: x (np.ndarray): Spatial grid points. R (float): Distance between nuclei. Returns: np.ndarray: External potential at each grid point. \u0026#34;\u0026#34;\u0026#34; center = (x[-1] + x[0]) / 2 a = 1e-5 # to avoid numeric issues v_ext = -1 / np.sqrt((x - (center - R/2))**2 + a) - 1 / np.sqrt((x - (center + R/2))**2 + a) return v_ext Here, R is the distance between the nuclei, and a is a small positive number to avoid division by zero. The potential is centered in our box.\nAdjusting the Number of Electrons H₂ has two electrons, so we set num_electrons = 2 in our run_dft_calculation function.\nCalculating the Total Energy To find the equilibrium bond length, we need to calculate the total energy, including the nuclear repulsion:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def calculate_total_energy(density: np.ndarray, v_ext: np.ndarray, v_eff: np.ndarray, eigenvalues: np.ndarray, x: np.ndarray, R: float) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34; Calculate the total energy of the H₂ system. Args: density (np.ndarray): Electron density. v_ext (np.ndarray): External potential. v_eff (np.ndarray): Effective potential. eigenvalues (np.ndarray): Kohn-Sham orbital energies. x (np.ndarray): Spatial grid points. R (float): Distance between nuclei. Returns: float: Total energy of the system. \u0026#34;\u0026#34;\u0026#34; # Kinetic energy T_s = np.sum(eigenvalues[:2]) # Sum of occupied orbital energies # Electron-nuclear attraction E_en = np.trapz(density * v_ext, x) # Hartree energy E_H = 0.5 * np.trapz(density * (v_eff - v_ext - exchange_correlation_potential_lda(density)), x) # Exchange-correlation energy E_xc = np.trapz(density * exchange_correlation_potential_lda(density), x) # Nuclear repulsion E_nn = 1 / R return T_s + E_en + E_H + E_xc + E_nn Finding the Equilibrium Bond Length We can find the equilibrium bond length by running our DFT calculation for different R values and finding the minimum energy:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def find_equilibrium_bond_length(x: np.ndarray, num_basis_functions: int) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34; Find the equilibrium bond length for H₂. Args: x (np.ndarray): Spatial grid points. num_basis_functions (int): Number of basis functions to use. Returns: float: Equilibrium bond length. \u0026#34;\u0026#34;\u0026#34; R_values = np.linspace(0.5, 2.5, 20) # in atomic units energies = [] for R in R_values: v_ext = external_potential_H2(x, R) density, eigenvalues, eigenvectors = scf_loop(x, 2, num_basis_functions, v_ext=v_ext) v_eff = calculate_effective_potential(density, x) energy = calculate_total_energy(density, v_ext, v_eff, eigenvalues, x, R) energies.append(energy) equilibrium_R = R_values[np.argmin(energies)] return equilibrium_R Running the Calculation Finally, we can run our calculation:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 x = np.linspace(-10, 10, 1000) num_basis_functions = 20 equilibrium_R = find_equilibrium_bond_length(x, num_basis_functions) print(f\u0026#34;Equilibrium bond length: {equilibrium_R:.3f} atomic units\u0026#34;) # Run DFT at equilibrium bond length v_ext = external_potential_H2(x, equilibrium_R) density, eigenvalues, eigenvectors = scf_loop(x, 2, num_basis_functions, v_ext=v_ext) v_eff = calculate_effective_potential(density, x) energy = calculate_total_energy(density, v_ext, v_eff, eigenvalues, x, equilibrium_R) print(f\u0026#34;Total energy at equilibrium: {energy:.6f} atomic units\u0026#34;) # Visualize results visualize_results(x, density, v_eff, eigenvectors, num_basis_functions) Conclusion In this blog post, we\u0026rsquo;ve implemented a basic 1D Density Functional Theory code in Python. We\u0026rsquo;ve covered the key components of DFT, including:\nBasis set definition using particle-in-a-box eigenfunctions Hamiltonian construction with kinetic and potential energy terms Density calculation from Kohn-Sham orbitals Effective potential calculation, including external, Hartree, and exchange-correlation potentials Self-consistent field loop for iterative solution of the Kohn-Sham equations Visualization of results This implementation serves as a starting point for understanding DFT calculations. In real-world applications, more sophisticated basis sets, exchange-correlation functionals, and numerical techniques would be used to handle more complex systems and achieve better accuracy and efficiency.\nSome potential improvements and extensions to this code could include:\nImplementing more advanced exchange-correlation functionals (e.g., GGA) Adding support for different external potentials Improving the efficiency of the SCF loop (e.g., using DIIS convergence acceleration) Calculating and visualizing additional properties (e.g., total energy, HOMO-LUMO gap) Extending the code to handle multiple non-interacting particles By working through this implementation, you\u0026rsquo;ve gained insight into the fundamental principles of DFT and how they can be translated into code. This knowledge forms a solid foundation for understanding more complex DFT implementations and their applications in computational chemistry and materials science.\nFurther Reading To deepen your understanding of DFT and its applications, you might want to explore the following resources:\n\u0026ldquo;Density Functional Theory: A Practical Introduction\u0026rdquo; by David Sholl and Janice A. Steckel \u0026ldquo;Electronic Structure: Basic Theory and Practical Methods\u0026rdquo; by Richard M. Martin \u0026ldquo;A Bird\u0026rsquo;s-Eye View of Density-Functional Theory\u0026rdquo; by Kieron Burke and Lucas O. Wagner (https://arxiv.org/abs/1308.5164) The Psi4NumPy project, which includes educational implementations of various quantum chemistry methods: https://github.com/psi4/psi4numpy In the next blog post, we\u0026rsquo;ll extend this code to handle 2D systems, introducing new challenges and opportunities for visualization and analysis of quantum systems.\n","permalink":"https://chc273.github.io/posts/2022-02-02-density-functional-theory-1/","summary":"Introduction to 1D Density Functional Theory Density Functional Theory (DFT) is a powerful computational method used in quantum chemistry and solid-state physics to investigate the electronic structure of many-body systems. While real-world applications of DFT often deal with three-dimensional systems, exploring DFT in one dimension provides an excellent starting point for understanding the fundamental concepts and implementation details.\nIn this blog post, we\u0026rsquo;ll walk through the process of building a 1D DFT code using Python.","title":"Building a 1D Density Functional Theory Code in Python"},{"content":"Fitting the parameters in STO-LG In computational chemistry, the Slater-type orbital (STO) more accurately describes the qualitative features of the molecular orbitals than Gaussian functions (GF). However, calculating the two-electron integral using STO can be costly. On the other hand, integrating GFs is relatively cheap. One way to solve this problem is to use a linear combination of GFs to approximate a STO. Such linear combination of Gaussian functions is called contracted Gaussian functions (CGF).\n$$\\phi_\\mu^{CGF}(\\vec{r}-\\vec{R}_A) = \\sum_{p=1}^L d_{p\\mu} \\phi_p^{GF} (\\alpha_{p\\mu}, \\vec{r} - \\vec{R}_A)$$\nwhere L is the length of the contraction, $d_{p\\mu}$ and $\\alpha_{p\\mu}$ are contraction coefficients and contraction exponents, respectively. Hence, the so-called STO-LG strategy uses L Gaussian-type orbitals to approximate one STO function.\nApproximating 1s Slater-type function using STO-LG The expressions for 1s STO and GF are\n\\begin{align} \\phi_{1s}^{STO} (\\zeta, \\vec{r}) = \\left( \\frac{\\zeta^3}{\\pi} \\right)^{1/2} e^{-\\zeta |\\vec{r}-{\\vec{R_A}}|} \\notag \\\\ \\phi_{1s}^{GF}(\\alpha, \\vec{r}) = \\left(\\frac{2\\alpha}{\\pi}\\right)^{3/4} e^{-\\alpha |\\vec{r}-{\\vec{R _A}}|^2} \\notag \\end{align}\nwhere both orbitals have their corresponding parameters. The goal is to find the $d_{p}$ and $\\alpha_{p}$ in the following equation\n$$ \\phi_{1s}^{STO} (\\zeta, \\vec{r}) = \\sum_p^L d_{p}\\phi_{1s}^{GF}(\\alpha_p, \\vec{r}) $$\nSTO-1G with $\\zeta = 1.0$ In the first case, we will show the process of fitting the simplest function STO-1G by assuming $\\zeta=1.0$ in the STO. Basically we will solve the following equation for $\\alpha_{11}$\n$$ \\phi_{1s}^{STO} (\\zeta=1.0, \\vec{r}) = \\phi_{1s}^{GF}(\\alpha_{11}, \\vec{r}) $$\n1 2 3 4 5 6 7 8 9 10 from sympy import * import numpy as np zeta, r, alpha, a11 = symbols(\u0026#34;zeta r alpha a11\u0026#34;, positive=True) # define the variables sto = (zeta **3 / pi) ** (1/2) * exp(-zeta * r) # general expression for one STO gf = (2 * alpha /pi) ** (3/4) * exp(-alpha * r**2) # general expression for one GF sto_1 = sto.subs(zeta, 1.0) # zeta = 1.0 gf_1 = gto.subs(alpha, a11) # alpha = a11 Instead of minimize the differences between the two functions, we will maximize the overlap between the GF and the STO following Szabo\u0026rsquo;s book.\n1 S = integrate(sto_1 * gf_1 * r**2, (r, 0, oo)) * 4 * pi # the overlap between STO(1.0, r) and GF(alpha, r) We will maximize this overlap in terms of $a_{11}$. Since we will use scipy, we turn the maximization problem into minimzation of the negative of the overlap S.\n1 2 3 4 # function to minimize def func(a): res = S.subs(a11, a[0]).evalf() return -res 1 from scipy.optimize import minimize 1 res = minimize(func, x0=[0.2], bounds=[(0.1, 1)]) 1 res.x[0] 0.2709502078346618 We get the $\\alpha_{11}$ value as 0.2709497296298158. This is almost identical to the result from Szabo\u0026rsquo;s book.\nShow the STO-1G plot 1 2 3 4 5 6 7 8 9 10 11 12 13 import matplotlib.pyplot as plt %matplotlib inline plt.rcParams[\u0026#39;font.size\u0026#39;] = 22 plt.rcParams[\u0026#39;font.family\u0026#39;] = \u0026#39;Arial\u0026#39; gf_a1 = lambdify(r, gf_1.subs(a11, res.x[0]), \u0026#34;numpy\u0026#34;) sto_1_np = lambdify(r, sto_1, \u0026#34;numpy\u0026#34;) r_np = np.linspace(0, 10, 101) plt.plot(r_np, gf_a1(r_np), label=\u0026#39;STO-1G\u0026#39;) plt.plot(r_np, sto_1_np(r_np), label=\u0026#39;STO\u0026#39;) plt.legend() plt.xlabel(\u0026#34;$r$\u0026#34;) \u0026lt;matplotlib.text.Text at 0x7fcc2089b898\u0026gt; The results are reasonably good.\nSTO-LG We will code the general procedure to calculate $L\u0026gt;1$ CGFs.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 from IPython.display import display, Math def get_gto(d, alpha): return d * (2 * alpha /pi) ** (3/4) * exp(-alpha * r**2) def get_symbols(L): ds = symbols(f\u0026#39;d:{L}\u0026#39;, positive=True) alphas = symbols(f\u0026#39;alpha:{L}\u0026#39;, positive=True) return ds, alphas class STOLG: def __init__(self, L=3, zta=1.0): self.L = L self.ds, self.alphas = get_symbols(L) self.GFs = [get_gto(d, a) for d, a in zip(self.ds, self.alphas)] self.GF_sum = sum(self.GFs) self.gg_int = integrate(self.GF_sum * self.GF_sum * r**2, (r, 0, oo)) * 4 * pi self.gg_int = self.gg_int.evalf() self.sto = sto.subs(zeta, zta) self.S = integrate(self.GF_sum * self.sto * r**2, (r, 0, oo)) * 4 * pi def fit(self): def _func(x): subs = {i: j for i, j in zip(self.ds[1:]+self.alphas, x)} d0_val = solve(self.gg_int.subs(subs) - 1) subs[self.ds[0]] = d0_val[0] val = self.S.subs(subs).evalf() # print(subs) self.subs = subs return -float(val) # initial guesses d_vals = np.linspace(0.1, 0.9, self.L-1).tolist() alpha_vals = np.linspace(0.1, 0.9, self.L).tolist() self.res = minimize(_func, x0=d_vals + alpha_vals, bounds=[(0, 10)] * (2 * self.L - 1)) return self.res @property def expression(self): expr = [] for i, j in zip(self.ds, self.alphas): expr.append(r\u0026#34;%.6f\\phi^{GF}(%.6f)\u0026#34; % (self.subs[i], self.subs[j])) return display(Math(\u0026#39;+\u0026#39;.join(expr))) @property def func(self): return lambdify(r, self.GF_sum.subs(self.subs), \u0026#34;numpy\u0026#34;) @property def funcs(self): return [lambdify(r, i.subs(self.subs), \u0026#34;numpy\u0026#34;) for i in self.GFs] def __call__(self, r): return self.func(r) def plot(self, r): plt.plot(r, self(r), \u0026#39;-\u0026#39;, label=f\u0026#39;STO-{self.L}G\u0026#39;) for i in range(self.L): plt.plot(r, self.funcs[i](r), \u0026#39;--\u0026#39;, label=f\u0026#39;GF-{i}\u0026#39;) plt.plot(r, sto_1_np(r), \u0026#39;-\u0026#39;, label=\u0026#39;STO\u0026#39;) plt.legend() STO-2G 1 2 sto2g = STOLG(L=2) sto2g.fit() fun: -0.9984197028799346 hess_inv: \u0026lt;3x3 LbfgsInvHessProduct with dtype=float64\u0026gt; jac: array([ 0.00000000e+00, 2.40918396e-06, -4.10782519e-07]) message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_\u0026lt;=_PGTOL' nfev: 96 nit: 19 status: 0 success: True x: array([0.43013353, 0.15162213, 0.85180271]) The overlap has reached 0.998 with only two GFs.\n1 sto2g.expression $$0.678908\\phi^{GF}(0.151622)+0.430134\\phi^{GF}(0.851803)$$\nThe expression above matches with Equation (3.220) in Szabo\u0026rsquo;s book.\n1 sto2g.plot(r_np) STO-3G 1 2 sto3g = STOLG(3) sto3g.fit() fun: -0.9998347361981794 hess_inv: \u0026lt;5x5 LbfgsInvHessProduct with dtype=float64\u0026gt; jac: array([5.09592368e-06, 1.04027897e-05, 1.17794663e-05, 5.55111512e-06, 2.22044605e-08]) message: b'CONVERGENCE: REL_REDUCTION_OF_F_\u0026lt;=_FACTR*EPSMCH' nfev: 276 nit: 39 status: 0 success: True x: array([0.53532369, 0.15432918, 0.10982016, 0.40578573, 2.22784233]) 1 sto3g.expression $$0.444642\\phi^{GF}(0.109820)+0.535324\\phi^{GF}(0.405786)+0.154329\\phi^{GF}(2.227842)$$\nAgain the STO-3G expression matches with Equation (3.221) in Szabo\u0026rsquo;s book.\n1 sto3g.plot(r_np) Summary In this notebook, I show how we can fit the parameters in the contracted Gaussian functions. The results are relatively sensitive to the initial guesses given the optimizers. I believe it will be more so if L further increases.\nI also see that with changing $\\zeta$, the optimizer gives me results different from the scaling relationships. It will be interesting to further investigate the cause.\n","permalink":"https://chc273.github.io/posts/2022-01-01-fitting-parameters-in-sto-lg/","summary":"Fitting the parameters in STO-LG In computational chemistry, the Slater-type orbital (STO) more accurately describes the qualitative features of the molecular orbitals than Gaussian functions (GF). However, calculating the two-electron integral using STO can be costly. On the other hand, integrating GFs is relatively cheap. One way to solve this problem is to use a linear combination of GFs to approximate a STO. Such linear combination of Gaussian functions is called contracted Gaussian functions (CGF).","title":"Fitting the parameters in STO-LG"},{"content":"Calculating energy of 1D Schrödinger equation using Sympy I was reading Szabo\u0026rsquo;s book \u0026ldquo;Modern Quantum Chemistry\u0026rdquo; and saw the exercise questions that seem to be solvable via programming. Hence I decided to give it a try and pick up sympy at the same time. I was a mathematica user back in undergrad, but have not used it ever since. Maybe sympy will just do the same trick.\nHere is the question (exercise 1.18 from the book)\nThe Schrödinger equation (in atomic units) of an electron moving in one dimension under the influence of the potential $$-\\delta(x)$$ is\n$$ \\left(-\\frac{1}{2}\\frac{d^2}{dx^2} - \\delta(x)\\right) | \\Phi\\rangle = \\epsilon | \\Phi\\rangle $$\nUse the variation method with the trial function\n$$ | \\tilde \\Phi \\rangle = N e^{-ax^2} $$\nto show that $$-\\pi^{-1}$$ is an upper bound to the exact group state energy (which is -0.5).\nFrom the variational principle, we know if the normalized wavefunction satisfies the appropriate boundary condition, then the expectation of the Hamiltonian is an upper bound to the exact ground state energy. In math expressions, if\n$$ \\langle\\tilde\\Phi|\\tilde\\Phi \\rangle = 1 $$\nthen\n$$ \\langle\\tilde\\Phi| \\mathcal{H} | \\tilde\\Phi \\rangle \\ge \\epsilon_0 $$\nI will show how we can use sympy to solve this problem\n1 2 3 4 5 6 7 8 from sympy import * # I dislike this way to import everything, but it seems to be common in sympy init_printing() x = symbols(\u0026#34;x\u0026#34;) # define the symbols N, a = symbols(\u0026#34;N a\u0026#34;, positive=True) # a should be positive to satisfy boundary cond at infinity phi = N * exp(-a * x ** 2) # this is our trial function 1. Normalization conditions We will need to normalize the wavefunction\n$$ \\langle\\tilde\\Phi|\\tilde\\Phi \\rangle = 1 $$\n1 2 3 phi2_int = integrate(phi * phi, (x, -oo, oo)) # the integration of phi * phi. Our function is real here phi2_int $$\\frac{\\sqrt{2} \\sqrt{\\pi} N^{2}}{2 \\sqrt{a}}$$\nThis expression equals to 1 from our normalization condition. Hence we can solve for $$N$$\n1 2 n_cond = solve(phi2_int-1) n_cond $$\\left [ \\left { N : \\frac{\\sqrt[4]{2} \\sqrt[4]{a}}{\\sqrt[4]{\\pi}}\\right }\\right ]$$\n2. Calculate the Hamiltonian with the trial function 1 2 3 4 5 6 term1 = integrate(phi * (-0.5 * diff(diff(phi, x), x)), (x, -oo, oo)) term2 = integrate(-phi * DiracDelta(x) * phi, (x, -oo, oo)) H = term1 + term2 H $$0.25 \\sqrt{2} \\sqrt{\\pi} N^{2} \\sqrt{a} - N^{2}$$\n3. Substitute the normalization condition 1 2 3 H_sol = H.subs(n_cond[0]) H_sol $$- \\frac{\\sqrt{2} \\sqrt{a}}{\\sqrt{\\pi}} + 0.5 a$$\nThis expression still contains $$a$$. To find the minimum of this equation, we will need to solve for $$a$$\n4. Minimize with respect to $$a$$ $$a$$ is minimal when $$\\partial H_{sol}/\\partial a = 0$$\n1 2 a_sol = solve(diff(H_sol, a))[0] a_sol $$0.636619772367581$$\nsubstitute $$a$$ solution into the solution for $$H$$, we get\n1 H_sol.subs({a: a_sol}).evalf() $$-0.318309886183791$$\nwhich is exactly $$-\\pi^{-1}$$\nIn summary, in this notebook, I show how we can use sympy to solve simple Schrödinger equation. Sometimes, using sympy can be unintuitive especially if the bounds of the variables are not properly set. In that case, you will get piecewise function results, and you will need to manually select the correct solutions.\nI found the use of expression oo to represent infinity quite interesting and brilliant.\n","permalink":"https://chc273.github.io/posts/2021-12-11-sympy-for-hamiltonian/","summary":"Calculating energy of 1D Schrödinger equation using Sympy I was reading Szabo\u0026rsquo;s book \u0026ldquo;Modern Quantum Chemistry\u0026rdquo; and saw the exercise questions that seem to be solvable via programming. Hence I decided to give it a try and pick up sympy at the same time. I was a mathematica user back in undergrad, but have not used it ever since. Maybe sympy will just do the same trick.\nHere is the question (exercise 1.","title":"Calculating energy of 1D Schrödinger equation using Sympy"},{"content":"Speed comparison among numpy, cython, numba and tensorflow 2.0 Recently I have been working on speeding up some codes in pymatgen for finding the atomic neighbors within a cutoff radius. I was searching online and found that cython is a rather powerful tool for accelerating python loops, and decided to give it a try.\nA common comparison for cython is numba and I have heard many good things about it. A less common competitor is the recently released tensorflow 2.0. In fact, back in the tensorflow 1.x era, I did some simple comparisons and found that the speed was in fact faster than numpy. The new tensorflow 2.0 is boasted to be 3x faster than tensorflow 1.x, and it makes me wonder how faster would tensorflow 2.0 be for some simple computing tasks.\nFunction decorate to record time I like to do simple things myself so that I know what exactly happens in the code. So I am writing a timeit decorator instead of using timeit package.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from time import time import functools def timeit(n=10): \u0026#34;\u0026#34;\u0026#34; Decorator to run function n times and print out the total time elapsed. \u0026#34;\u0026#34;\u0026#34; def dec(func): @functools.wraps(func) def wrapped(*args, **kwargs): t0 = time() for i in range(n): func(*args, **kwargs) print(\u0026#34;%s iterated %d times\\nTime elapsed %.3fs\\n\u0026#34; % ( func.__name__, n, time() - t0)) return wrapped return dec Computing functions using different methods Here I am computing\n\\[matrix[i, j] = i^2 + j^2\\]\nfor a matrix of size [m, n]\n1 2 3 4 5 6 # import numba, tensorflow and numpy, load cython import numba import tensorflow as tf import numpy as np %load_ext cython 1 2 3 4 5 6 7 8 @tf.function def compute_tf(m, n): print(\u0026#39;Tracing \u0026#39;, m, n) x1 = tf.range(0, m-1, 1) ** 2 x2 = tf.range(0, n-1, 1) ** 2 return x1[:, None] + x2[None, :] compute_tf(tf.constant(1), tf.constant(1)) # trace once Tracing Tensor(\u0026quot;m:0\u0026quot;, shape=(), dtype=int32) Tensor(\u0026quot;n:0\u0026quot;, shape=(), dtype=int32) \u0026lt;tf.Tensor: id=261, shape=(0, 0), dtype=int32, numpy=array([], shape=(0, 0), dtype=int32)\u0026gt; I used the tf.function decorate to define the graph and avoided repeated tracing the graph by using tf.constant as input and perform the initial graph tracing. You will see that running this function will not invoke the print function. It is only traced once\n1 2 3 4 def compute_numpy(m, n): x1 = np.linspace(0., m-1, m) ** 2 x2 = np.linspace(0., n-1, n) ** 2 return x1[:, None] + x2[None, :] 1 2 3 4 5 6 7 8 9 @numba.njit def compute_numba(m, n): x = np.empty((m, n)) for i in range(m): for j in range(n): x[i, j] = i**2 + j**2 return x compute_numba(1, 1) # JIT compile first 1 2 3 4 5 6 7 8 @numba.njit(parallel=True) def compute_numba_parallel(m, n): x = np.empty((m, n)) for i in numba.prange(m): for j in numba.prange(n): x[i, j] = i**2 + j**2 return x compute_numba_parallel(1, 1) # JIT compile first array([[0.]]) Numpy and numba are almost the same. numba is really handy in terms of turning on parallel computations.\n1 2 3 4 5 6 7 8 9 10 11 12 13 %%cython cimport cython import numpy as np cimport numpy as np @cython.boundscheck(False) @cython.wraparound(False) def compute_cython(int m, int n): cdef long [:, ::1] x = np.empty((m, n), dtype=int) cdef int i, j for i in range(m): for j in range(n): x[i, j] = i*i +j*j return x cython needs more work and i am delegating the memory management to numpy here and use memoryview x. Basically it is like C. Note that cython can also turn on parallel computations like numba by using cython.parallel.prange. However it does require openmp, which does not ship with clang compiler in macos. So I am not testing the parallel version here.\nResults 1 2 3 4 5 6 7 8 9 m = 2000 n = 10000 n_loop = 10 timeit(n=n_loop)(compute_numpy)(m, n) timeit(n=n_loop)(compute_numba)(m, n) timeit(n=n_loop)(compute_numba_parallel)(m, n) timeit(n=n_loop)(compute_cython)(m, n) timeit(n=n_loop)(compute_tf)(tf.constant(m), tf.constant(n)) compute_numpy iterated 10 times Time elapsed 0.971s compute_numba iterated 10 times Time elapsed 1.110s compute_numba_parallel iterated 10 times Time elapsed 0.651s compute_cython iterated 10 times Time elapsed 1.098s compute_tf iterated 10 times Time elapsed 0.190s Conclusion Tensorflow 2.0 is amazing.\n","permalink":"https://chc273.github.io/posts/2019-10-04-speed-comparison-among-numpy-cython-numba-and-tensorflow/","summary":"Speed comparison among numpy, cython, numba and tensorflow 2.0 Recently I have been working on speeding up some codes in pymatgen for finding the atomic neighbors within a cutoff radius. I was searching online and found that cython is a rather powerful tool for accelerating python loops, and decided to give it a try.\nA common comparison for cython is numba and I have heard many good things about it. A less common competitor is the recently released tensorflow 2.","title":"Speed comparison among numpy, cython, numba and tensorflow 2.0"}]