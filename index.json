[{"content":"Fitting the parameters in STO-LG In computational chemistry, the Slater-type orbital (STO) more accurately describes the qualitative features of the molecular orbitals than Gaussian functions (GF). However, calculating the two-electron integral using STO can be costly. On the other hand, integrating GFs is relatively cheap. One way to solve this problem is to use a linear combination of GFs to approximate a STO. Such linear combination of Gaussian functions is called contracted Gaussian functions (CGF).\n$$\\phi_\\mu^{CGF}(\\vec{r}-\\vec{R}_A) = \\sum_{p=1}^L d_{p\\mu} \\phi_p^{GF} (\\alpha_{p\\mu}, \\vec{r} - \\vec{R}_A)$$\nwhere L is the length of the contraction, $d_{p\\mu}$ and $\\alpha_{p\\mu}$ are contraction coefficients and contraction exponents, respectively. Hence, the so-called STO-LG strategy uses L Gaussian-type orbitals to approximate one STO function.\nApproximating 1s Slater-type function using STO-LG The expressions for 1s STO and GF are\n\\begin{align} \\phi_{1s}^{STO} (\\zeta, \\vec{r}) = \\left( \\frac{\\zeta^3}{\\pi} \\right)^{1/2} e^{-\\zeta |\\vec{r}-{\\vec{R_A}}|} \\notag \\\\ \\phi_{1s}^{GF}(\\alpha, \\vec{r}) = \\left(\\frac{2\\alpha}{\\pi}\\right)^{3/4} e^{-\\alpha |\\vec{r}-{\\vec{R _A}}|^2} \\notag \\end{align}\nwhere both orbitals have their corresponding parameters. The goal is to find the $d_{p}$ and $\\alpha_{p}$ in the following equation\n$$ \\phi_{1s}^{STO} (\\zeta, \\vec{r}) = \\sum_p^L d_{p}\\phi_{1s}^{GF}(\\alpha_p, \\vec{r}) $$\nSTO-1G with $\\zeta = 1.0$ In the first case, we will show the process of fitting the simplest function STO-1G by assuming $\\zeta=1.0$ in the STO. Basically we will solve the following equation for $\\alpha_{11}$\n$$ \\phi_{1s}^{STO} (\\zeta=1.0, \\vec{r}) = \\phi_{1s}^{GF}(\\alpha_{11}, \\vec{r}) $$\nfrom sympy import * import numpy as np zeta, r, alpha, a11 = symbols(\u0026#34;zeta r alpha a11\u0026#34;, positive=True) # define the variables sto = (zeta **3 / pi) ** (1/2) * exp(-zeta * r) # general expression for one STO gf = (2 * alpha /pi) ** (3/4) * exp(-alpha * r**2) # general expression for one GF sto_1 = sto.subs(zeta, 1.0) # zeta = 1.0 gf_1 = gto.subs(alpha, a11) # alpha = a11 Instead of minimize the differences between the two functions, we will maximize the overlap between the GF and the STO following Szabo\u0026rsquo;s book.\nS = integrate(sto_1 * gf_1 * r**2, (r, 0, oo)) * 4 * pi # the overlap between STO(1.0, r) and GF(alpha, r) We will maximize this overlap in terms of $a_{11}$. Since we will use scipy, we turn the maximization problem into minimzation of the negative of the overlap S.\n# function to minimize def func(a): res = S.subs(a11, a[0]).evalf() return -res from scipy.optimize import minimize res = minimize(func, x0=[0.2], bounds=[(0.1, 1)]) res.x[0] 0.2709502078346618 We get the $\\alpha_{11}$ value as 0.2709497296298158. This is almost identical to the result from Szabo\u0026rsquo;s book.\nShow the STO-1G plot import matplotlib.pyplot as plt %matplotlib inline plt.rcParams[\u0026#39;font.size\u0026#39;] = 22 plt.rcParams[\u0026#39;font.family\u0026#39;] = \u0026#39;Arial\u0026#39; gf_a1 = lambdify(r, gf_1.subs(a11, res.x[0]), \u0026#34;numpy\u0026#34;) sto_1_np = lambdify(r, sto_1, \u0026#34;numpy\u0026#34;) r_np = np.linspace(0, 10, 101) plt.plot(r_np, gf_a1(r_np), label=\u0026#39;STO-1G\u0026#39;) plt.plot(r_np, sto_1_np(r_np), label=\u0026#39;STO\u0026#39;) plt.legend() plt.xlabel(\u0026#34;$r$\u0026#34;) \u0026lt;matplotlib.text.Text at 0x7fcc2089b898\u0026gt; The results are reasonably good.\nSTO-LG We will code the general procedure to calculate $L\u0026gt;1$ CGFs.\nfrom IPython.display import display, Math def get_gto(d, alpha): return d * (2 * alpha /pi) ** (3/4) * exp(-alpha * r**2) def get_symbols(L): ds = symbols(f\u0026#39;d:{L}\u0026#39;, positive=True) alphas = symbols(f\u0026#39;alpha:{L}\u0026#39;, positive=True) return ds, alphas class STOLG: def __init__(self, L=3, zta=1.0): self.L = L self.ds, self.alphas = get_symbols(L) self.GFs = [get_gto(d, a) for d, a in zip(self.ds, self.alphas)] self.GF_sum = sum(self.GFs) self.gg_int = integrate(self.GF_sum * self.GF_sum * r**2, (r, 0, oo)) * 4 * pi self.gg_int = self.gg_int.evalf() self.sto = sto.subs(zeta, zta) self.S = integrate(self.GF_sum * self.sto * r**2, (r, 0, oo)) * 4 * pi def fit(self): def _func(x): subs = {i: j for i, j in zip(self.ds[1:]+self.alphas, x)} d0_val = solve(self.gg_int.subs(subs) - 1) subs[self.ds[0]] = d0_val[0] val = self.S.subs(subs).evalf() # print(subs) self.subs = subs return -float(val) # initial guesses d_vals = np.linspace(0.1, 0.9, self.L-1).tolist() alpha_vals = np.linspace(0.1, 0.9, self.L).tolist() self.res = minimize(_func, x0=d_vals + alpha_vals, bounds=[(0, 10)] * (2 * self.L - 1)) return self.res @property def expression(self): expr = [] for i, j in zip(self.ds, self.alphas): expr.append(r\u0026#34;%.6f\\phi^{GF}(%.6f)\u0026#34; % (self.subs[i], self.subs[j])) return display(Math(\u0026#39;+\u0026#39;.join(expr))) @property def func(self): return lambdify(r, self.GF_sum.subs(self.subs), \u0026#34;numpy\u0026#34;) @property def funcs(self): return [lambdify(r, i.subs(self.subs), \u0026#34;numpy\u0026#34;) for i in self.GFs] def __call__(self, r): return self.func(r) def plot(self, r): plt.plot(r, self(r), \u0026#39;-\u0026#39;, label=f\u0026#39;STO-{self.L}G\u0026#39;) for i in range(self.L): plt.plot(r, self.funcs[i](r), \u0026#39;--\u0026#39;, label=f\u0026#39;GF-{i}\u0026#39;) plt.plot(r, sto_1_np(r), \u0026#39;-\u0026#39;, label=\u0026#39;STO\u0026#39;) plt.legend() STO-2G sto2g = STOLG(L=2) sto2g.fit() fun: -0.9984197028799346 hess_inv: \u0026lt;3x3 LbfgsInvHessProduct with dtype=float64\u0026gt; jac: array([ 0.00000000e+00, 2.40918396e-06, -4.10782519e-07]) message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_\u0026lt;=_PGTOL' nfev: 96 nit: 19 status: 0 success: True x: array([0.43013353, 0.15162213, 0.85180271]) The overlap has reached 0.998 with only two GFs.\nsto2g.expression $$0.678908\\phi^{GF}(0.151622)+0.430134\\phi^{GF}(0.851803)$$\nThe expression above matches with Equation (3.220) in Szabo\u0026rsquo;s book.\nsto2g.plot(r_np) STO-3G sto3g = STOLG(3) sto3g.fit() fun: -0.9998347361981794 hess_inv: \u0026lt;5x5 LbfgsInvHessProduct with dtype=float64\u0026gt; jac: array([5.09592368e-06, 1.04027897e-05, 1.17794663e-05, 5.55111512e-06, 2.22044605e-08]) message: b'CONVERGENCE: REL_REDUCTION_OF_F_\u0026lt;=_FACTR*EPSMCH' nfev: 276 nit: 39 status: 0 success: True x: array([0.53532369, 0.15432918, 0.10982016, 0.40578573, 2.22784233]) sto3g.expression $$0.444642\\phi^{GF}(0.109820)+0.535324\\phi^{GF}(0.405786)+0.154329\\phi^{GF}(2.227842)$$\nAgain the STO-3G expression matches with Equation (3.221) in Szabo\u0026rsquo;s book.\nsto3g.plot(r_np) Summary In this notebook, I show how we can fit the parameters in the contracted Gaussian functions. The results are relatively sensitive to the initial guesses given the optimizers. I believe it will be more so if L further increases.\nI also see that with changing $\\zeta$, the optimizer gives me results different from the scaling relationships. It will be interesting to further investigate the cause.\n","permalink":"https://chc273.github.io/posts/2022-01-01-fitting-parameters-in-sto-lg/","summary":"Fitting the parameters in STO-LG In computational chemistry, the Slater-type orbital (STO) more accurately describes the qualitative features of the molecular orbitals than Gaussian functions (GF). However, calculating the two-electron integral using STO can be costly. On the other hand, integrating GFs is relatively cheap. One way to solve this problem is to use a linear combination of GFs to approximate a STO. Such linear combination of Gaussian functions is called contracted Gaussian functions (CGF).","title":"Fitting the parameters in STO-LG"},{"content":"Calculating energy of 1D Schrödinger equation using Sympy I was reading Szabo\u0026rsquo;s book \u0026ldquo;Modern Quantum Chemistry\u0026rdquo; and saw the exercise questions that seem to be solvable via programming. Hence I decided to give it a try and pick up sympy at the same time. I was a mathematica user back in undergrad, but have not used it ever since. Maybe sympy will just do the same trick.\nHere is the question (exercise 1.18 from the book)\nThe Schrödinger equation (in atomic units) of an electron moving in one dimension under the influence of the potential $$-\\delta(x)$$ is\n$$ \\left(-\\frac{1}{2}\\frac{d^2}{dx^2} - \\delta(x)\\right) | \\Phi\\rangle = \\epsilon | \\Phi\\rangle $$\nUse the variation method with the trial function\n$$ | \\tilde \\Phi \\rangle = N e^{-ax^2} $$\nto show that $$-\\pi^{-1}$$ is an upper bound to the exact group state energy (which is -0.5).\nFrom the variational principle, we know if the normalized wavefunction satisfies the appropriate boundary condition, then the expectation of the Hamiltonian is an upper bound to the exact ground state energy. In math expressions, if\n$$ \\langle\\tilde\\Phi|\\tilde\\Phi \\rangle = 1 $$\nthen\n$$ \\langle\\tilde\\Phi| \\mathcal{H} | \\tilde\\Phi \\rangle \\ge \\epsilon_0 $$\nI will show how we can use sympy to solve this problem\nfrom sympy import * # I dislike this way to import everything, but it seems to be common in sympy init_printing() x = symbols(\u0026#34;x\u0026#34;) # define the symbols N, a = symbols(\u0026#34;N a\u0026#34;, positive=True) # a should be positive to satisfy boundary cond at infinity phi = N * exp(-a * x ** 2) # this is our trial function 1. Normalization conditions We will need to normalize the wavefunction\n$$ \\langle\\tilde\\Phi|\\tilde\\Phi \\rangle = 1 $$\nphi2_int = integrate(phi * phi, (x, -oo, oo)) # the integration of phi * phi. Our function is real here phi2_int $$\\frac{\\sqrt{2} \\sqrt{\\pi} N^{2}}{2 \\sqrt{a}}$$\nThis expression equals to 1 from our normalization condition. Hence we can solve for $$N$$\nn_cond = solve(phi2_int-1) n_cond $$\\left [ \\left { N : \\frac{\\sqrt[4]{2} \\sqrt[4]{a}}{\\sqrt[4]{\\pi}}\\right }\\right ]$$\n2. Calculate the Hamiltonian with the trial function term1 = integrate(phi * (-0.5 * diff(diff(phi, x), x)), (x, -oo, oo)) term2 = integrate(-phi * DiracDelta(x) * phi, (x, -oo, oo)) H = term1 + term2 H $$0.25 \\sqrt{2} \\sqrt{\\pi} N^{2} \\sqrt{a} - N^{2}$$\n3. Substitute the normalization condition H_sol = H.subs(n_cond[0]) H_sol $$- \\frac{\\sqrt{2} \\sqrt{a}}{\\sqrt{\\pi}} + 0.5 a$$\nThis expression still contains $$a$$. To find the minimum of this equation, we will need to solve for $$a$$\n4. Minimize with respect to $$a$$ $$a$$ is minimal when $$\\partial H_{sol}/\\partial a = 0$$\na_sol = solve(diff(H_sol, a))[0] a_sol $$0.636619772367581$$\nsubstitute $$a$$ solution into the solution for $$H$$, we get\nH_sol.subs({a: a_sol}).evalf() $$-0.318309886183791$$\nwhich is exactly $$-\\pi^{-1}$$\nIn summary, in this notebook, I show how we can use sympy to solve simple Schrödinger equation. Sometimes, using sympy can be unintuitive especially if the bounds of the variables are not properly set. In that case, you will get piecewise function results, and you will need to manually select the correct solutions.\nI found the use of expression oo to represent infinity quite interesting and brilliant.\n","permalink":"https://chc273.github.io/posts/2021-12-11-sympy-for-hamiltonian/","summary":"Calculating energy of 1D Schrödinger equation using Sympy I was reading Szabo\u0026rsquo;s book \u0026ldquo;Modern Quantum Chemistry\u0026rdquo; and saw the exercise questions that seem to be solvable via programming. Hence I decided to give it a try and pick up sympy at the same time. I was a mathematica user back in undergrad, but have not used it ever since. Maybe sympy will just do the same trick.\nHere is the question (exercise 1.","title":"Calculating energy of 1D Schrödinger equation using Sympy"},{"content":"Speed comparison among numpy, cython, numba and tensorflow 2.0 Recently I have been working on speeding up some codes in pymatgen for finding the atomic neighbors within a cutoff radius. I was searching online and found that cython is a rather powerful tool for accelerating python loops, and decided to give it a try.\nA common comparison for cython is numba and I have heard many good things about it. A less common competitor is the recently released tensorflow 2.0. In fact, back in the tensorflow 1.x era, I did some simple comparisons and found that the speed was in fact faster than numpy. The new tensorflow 2.0 is boasted to be 3x faster than tensorflow 1.x, and it makes me wonder how faster would tensorflow 2.0 be for some simple computing tasks.\nFunction decorate to record time I like to do simple things myself so that I know what exactly happens in the code. So I am writing a timeit decorator instead of using timeit package.\nfrom time import time import functools def timeit(n=10): \u0026#34;\u0026#34;\u0026#34; Decorator to run function n times and print out the total time elapsed. \u0026#34;\u0026#34;\u0026#34; def dec(func): @functools.wraps(func) def wrapped(*args, **kwargs): t0 = time() for i in range(n): func(*args, **kwargs) print(\u0026#34;%s iterated %d times\\nTime elapsed %.3fs\\n\u0026#34; % ( func.__name__, n, time() - t0)) return wrapped return dec Computing functions using different methods Here I am computing\n\\[matrix[i, j] = i^2 + j^2\\]\nfor a matrix of size [m, n]\n# import numba, tensorflow and numpy, load cython import numba import tensorflow as tf import numpy as np %load_ext cython @tf.function def compute_tf(m, n): print(\u0026#39;Tracing \u0026#39;, m, n) x1 = tf.range(0, m-1, 1) ** 2 x2 = tf.range(0, n-1, 1) ** 2 return x1[:, None] + x2[None, :] compute_tf(tf.constant(1), tf.constant(1)) # trace once Tracing Tensor(\u0026quot;m:0\u0026quot;, shape=(), dtype=int32) Tensor(\u0026quot;n:0\u0026quot;, shape=(), dtype=int32) \u0026lt;tf.Tensor: id=261, shape=(0, 0), dtype=int32, numpy=array([], shape=(0, 0), dtype=int32)\u0026gt; I used the tf.function decorate to define the graph and avoided repeated tracing the graph by using tf.constant as input and perform the initial graph tracing. You will see that running this function will not invoke the print function. It is only traced once\ndef compute_numpy(m, n): x1 = np.linspace(0., m-1, m) ** 2 x2 = np.linspace(0., n-1, n) ** 2 return x1[:, None] + x2[None, :] @numba.njit def compute_numba(m, n): x = np.empty((m, n)) for i in range(m): for j in range(n): x[i, j] = i**2 + j**2 return x compute_numba(1, 1) # JIT compile first @numba.njit(parallel=True) def compute_numba_parallel(m, n): x = np.empty((m, n)) for i in numba.prange(m): for j in numba.prange(n): x[i, j] = i**2 + j**2 return x compute_numba_parallel(1, 1) # JIT compile first array([[0.]]) Numpy and numba are almost the same. numba is really handy in terms of turning on parallel computations.\n%%cython cimport cython import numpy as np cimport numpy as np @cython.boundscheck(False) @cython.wraparound(False) def compute_cython(int m, int n): cdef long [:, ::1] x = np.empty((m, n), dtype=int) cdef int i, j for i in range(m): for j in range(n): x[i, j] = i*i +j*j return x cython needs more work and i am delegating the memory management to numpy here and use memoryview x. Basically it is like C. Note that cython can also turn on parallel computations like numba by using cython.parallel.prange. However it does require openmp, which does not ship with clang compiler in macos. So I am not testing the parallel version here.\nResults m = 2000 n = 10000 n_loop = 10 timeit(n=n_loop)(compute_numpy)(m, n) timeit(n=n_loop)(compute_numba)(m, n) timeit(n=n_loop)(compute_numba_parallel)(m, n) timeit(n=n_loop)(compute_cython)(m, n) timeit(n=n_loop)(compute_tf)(tf.constant(m), tf.constant(n)) compute_numpy iterated 10 times Time elapsed 0.971s compute_numba iterated 10 times Time elapsed 1.110s compute_numba_parallel iterated 10 times Time elapsed 0.651s compute_cython iterated 10 times Time elapsed 1.098s compute_tf iterated 10 times Time elapsed 0.190s Conclusion Tensorflow 2.0 is amazing.\n","permalink":"https://chc273.github.io/posts/2019-10-04-speed-comparison-among-numpy-cython-numba-and-tensorflow/","summary":"Speed comparison among numpy, cython, numba and tensorflow 2.0 Recently I have been working on speeding up some codes in pymatgen for finding the atomic neighbors within a cutoff radius. I was searching online and found that cython is a rather powerful tool for accelerating python loops, and decided to give it a try.\nA common comparison for cython is numba and I have heard many good things about it. A less common competitor is the recently released tensorflow 2.","title":"Speed comparison among numpy, cython, numba and tensorflow 2.0"}]