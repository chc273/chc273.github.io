<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>tensorflow on Chi&#39;s blog</title>
    <link>https://chc273.github.io/tags/tensorflow/</link>
    <description>Recent content in tensorflow on Chi&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 04 Oct 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://chc273.github.io/tags/tensorflow/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Speed comparison among numpy, cython, numba and tensorflow 2.0</title>
      <link>https://chc273.github.io/posts/2019-10-04-speed-comparison-among-numpy-cython-numba-and-tensorflow/</link>
      <pubDate>Fri, 04 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://chc273.github.io/posts/2019-10-04-speed-comparison-among-numpy-cython-numba-and-tensorflow/</guid>
      <description>Speed comparison among numpy, cython, numba and tensorflow 2.0 Recently I have been working on speeding up some codes in pymatgen for finding the atomic neighbors within a cutoff radius. I was searching online and found that cython is a rather powerful tool for accelerating python loops, and decided to give it a try.
A common comparison for cython is numba and I have heard many good things about it. A less common competitor is the recently released tensorflow 2.</description>
      <content:encoded><![CDATA[<h2 id="speed-comparison-among-numpy-cython-numba-and-tensorflow-20">Speed comparison among numpy, cython, numba and tensorflow 2.0</h2>
<p>Recently I have been working on speeding up some codes in pymatgen for finding the atomic neighbors within a cutoff radius. I was searching online and found that <code>cython</code> is a rather powerful tool for accelerating python loops, and decided to give it a try.</p>
<p>A common comparison for <code>cython</code> is <code>numba</code> and I have heard many good things about it. A less common competitor is the recently released <code>tensorflow 2.0</code>. In fact, back in the <code>tensorflow 1.x</code> era, I did some simple comparisons and found that the speed was in fact faster than <code>numpy</code>. The new <code>tensorflow 2.0</code> is boasted to be 3x faster than <code>tensorflow 1.x</code>, and it makes me wonder how faster would <code>tensorflow 2.0</code> be for some simple computing tasks.</p>
<h3 id="function-decorate-to-record-time">Function decorate to record time</h3>
<p>I like to do simple things myself so that I know what exactly happens in the code. So I am writing a timeit decorator instead of using <code>timeit</code> package.</p>
<pre><code class="language-python">from time import time
import functools

def timeit(n=10):
    &quot;&quot;&quot;
    Decorator to run function n times and print out the total time elapsed.
    &quot;&quot;&quot;
    def dec(func):
        @functools.wraps(func)
        def wrapped(*args, **kwargs):
            t0 = time()
            for i in range(n):
                func(*args, **kwargs)
            print(&quot;%s iterated %d times\nTime elapsed %.3fs\n&quot; % (
                func.__name__, n, time() - t0))
        return wrapped
    return dec
</code></pre>
<h3 id="computing-functions-using-different-methods">Computing functions using different methods</h3>
<p>Here I am computing</p>
<p>\[matrix[i, j] = i^2 + j^2\]</p>
<p>for a matrix of size <code>[m, n]</code></p>
<pre><code class="language-python"># import numba, tensorflow and numpy, load cython
import numba
import tensorflow as tf
import numpy as np

%load_ext cython
</code></pre>
<pre><code class="language-python">@tf.function
def compute_tf(m, n):
    print('Tracing ',  m,  n)
    x1 = tf.range(0, m-1, 1) ** 2
    x2 = tf.range(0, n-1, 1) ** 2
    return x1[:, None] + x2[None, :]

compute_tf(tf.constant(1), tf.constant(1)) # trace once
</code></pre>
<pre><code>Tracing  Tensor(&quot;m:0&quot;, shape=(), dtype=int32) Tensor(&quot;n:0&quot;, shape=(), dtype=int32)
&lt;tf.Tensor: id=261, shape=(0, 0), dtype=int32, numpy=array([], shape=(0, 0), dtype=int32)&gt;
</code></pre>
<p>I used the <code>tf.function</code> decorate to define the graph and avoided repeated tracing the graph by using <code>tf.constant</code> as input and perform the initial graph tracing. You will see that running this function will not invoke the <code>print</code> function. It is only traced once</p>
<pre><code class="language-python">def compute_numpy(m, n):
    x1 = np.linspace(0., m-1, m) ** 2
    x2 = np.linspace(0., n-1, n) ** 2
    return x1[:, None] + x2[None, :]
</code></pre>
<pre><code class="language-python">@numba.njit
def compute_numba(m, n):
    x = np.empty((m, n))
    for i in range(m):
        for j in range(n):
            x[i, j] =  i**2 + j**2
    return x

compute_numba(1, 1) # JIT compile first
</code></pre>
<pre><code class="language-python">@numba.njit(parallel=True)
def compute_numba_parallel(m, n):
    x = np.empty((m, n))
    for i in numba.prange(m):
        for j in numba.prange(n):
            x[i, j] =  i**2 + j**2
    return x
compute_numba_parallel(1, 1) # JIT compile first
</code></pre>
<pre><code>array([[0.]])
</code></pre>
<p>Numpy and numba are almost the same. <code>numba</code> is really handy in terms of turning on parallel computations.</p>
<pre><code class="language-python">%%cython
cimport cython
import numpy as np
cimport numpy as np
@cython.boundscheck(False)
@cython.wraparound(False)
def compute_cython(int m, int n):
    cdef long [:, ::1] x = np.empty((m, n), dtype=int)
    cdef int i, j
    for i in range(m):
        for j in range(n):
            x[i, j] = i*i +j*j
    return x
</code></pre>
<p><code>cython</code> needs more work and i am delegating the memory management to <code>numpy</code> here and use <code>memoryview</code> x. Basically it is like <code>C</code>. Note that <code>cython</code> can also turn on parallel computations like <code>numba</code> by using <code>cython.parallel.prange</code>. However it does require <code>openmp</code>, which does not ship with <code>clang</code> compiler in macos. So I am not testing the parallel version here.</p>
<h3 id="results">Results</h3>
<pre><code class="language-python">m = 2000
n = 10000
n_loop = 10

timeit(n=n_loop)(compute_numpy)(m, n)
timeit(n=n_loop)(compute_numba)(m, n)
timeit(n=n_loop)(compute_numba_parallel)(m, n)
timeit(n=n_loop)(compute_cython)(m, n)
timeit(n=n_loop)(compute_tf)(tf.constant(m), tf.constant(n))
</code></pre>
<pre><code>compute_numpy iterated 10 times
Time elapsed 0.971s

compute_numba iterated 10 times
Time elapsed 1.110s

compute_numba_parallel iterated 10 times
Time elapsed 0.651s

compute_cython iterated 10 times
Time elapsed 1.098s

compute_tf iterated 10 times
Time elapsed 0.190s
</code></pre>
<h3 id="conclusion">Conclusion</h3>
<p><code>Tensorflow 2.0</code> is amazing.</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
